{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moukouel/Notebooks/blob/main/PyWord.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OOcqnT2H4M_",
        "outputId": "a92eeef4-b366-4ecf-ab8e-9d6f5fc4c10f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import Module, Sequential\n",
        "import requests\n",
        "\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "print(th.__version__)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_Hvk2r_H4NH",
        "outputId": "2fbbb486-8b9d-4794-c20b-2639afa07254"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "<class 'str'>\n",
            "ï»¿The Project Gutenberg eBook of The Complete Works\n",
            "5575053\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "print(th.cuda.is_available())\n",
        "# Download the file\n",
        "url = 'https://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Read and decode the text\n",
        "text = response.content.decode(encoding='utf-8')\n",
        "\n",
        "# Check the type and print the first 50 characters\n",
        "print(type(text))  # <class 'str'>\n",
        "print(text[:50])\n",
        "\n",
        "# Get the length of the text\n",
        "print(len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "317qD4b4H4NH",
        "outputId": "4fdf309f-2113-4e91-8876-87a2f042fff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " the project gutenberg ebook of the complete works\n",
            "5306848\n"
          ]
        }
      ],
      "source": [
        "#Preprocess the text\n",
        "def preprocess_text(text):\n",
        "    # Remove non-ASCII characters\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    return text\n",
        "# Preprocess the text\n",
        "text = preprocess_text(text)\n",
        "# Check the first 50 characters after preprocessing\n",
        "print(text[:50])\n",
        "# Get the length of the preprocessed text\n",
        "print(len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee9oSli5H4NI",
        "outputId": "38f5c870-4af6-418d-ec8b-ea45dd594398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 tokens: ['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'complete', 'works', 'of', 'william']\n",
            "Total number of tokens: 1214698\n",
            "Number of unique tokens: 29377\n",
            "Vocabulary size: 29377\n",
            "First 10 sequences: [15985, 20744, 18673, 12497, 25576, 15985, 14172, 8693, 25576, 15069]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Tokenization and vocabulary building\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "tokens = word_tokenize(text)  # Tokenize the text into words\n",
        "\n",
        "# Check the first 10 tokens\n",
        "print(\"First 10 tokens:\", tokens[:10])\n",
        "\n",
        "# Check the number of tokens\n",
        "print(\"Total number of tokens:\", len(tokens))\n",
        "\n",
        "# Count the number of unique tokens\n",
        "unique_tokens = list(set(tokens))  # Create a list of unique tokens\n",
        "print(\"Number of unique tokens:\", len(unique_tokens))\n",
        "\n",
        "# Construct the vocabulary\n",
        "# Map each unique token to an index\n",
        "vocab = {token: idx for idx, token in enumerate(unique_tokens)}\n",
        "\n",
        "# Convert text to sequences of integers\n",
        "sequences = [vocab[token] for token in tokens]\n",
        "#Convert sequences into words\n",
        "reverse_vocab = {idx: token for token, idx in vocab.items()}\n",
        "# Convert sequences back to words\n",
        "decoded_sequences = [reverse_vocab[idx] for idx in sequences]\n",
        "\n",
        "# Vocabulary and sequence details\n",
        "print(\"Vocabulary size:\", len(vocab))\n",
        "print(\"First 10 sequences:\", sequences[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84MnEaBPH4NJ",
        "outputId": "bb6b3323-6d79-405e-bb57-b35a73f27b6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set: torch.Size([850286, 3]) torch.Size([850286])\n",
            "Validation Set: torch.Size([364409, 3]) torch.Size([364409])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create input-output pairs for training (next-word prediction task)\n",
        "X, y = [], []\n",
        "max_len = 3  # Define a fixed sequence length\n",
        "for i in range(max_len, len(sequences)):\n",
        "    X.append(sequences[i-max_len:i])  # Input: previous `max_len` words\n",
        "    y.append(sequences[i])           # Output: next word (target)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X = th.tensor(X, dtype=th.long)\n",
        "y = th.tensor(y, dtype=th.long)\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Print dataset shapes\n",
        "print(\"Training Set:\", X_train.shape, y_train.shape)\n",
        "print(\"Validation Set:\", X_val.shape, y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4duiF1GH4NJ",
        "outputId": "ad8be267-d98a-4464-ee4c-1eb1b02c619b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRUModel(\n",
            "  (embedding): Embedding(29377, 128)\n",
            "  (gru): GRU(128, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=29377, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ==========================================\n",
        "# Step 3: Build the RNN Model with GRU\n",
        "# ==========================================\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, gru_units):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Embedding layer\n",
        "\n",
        "        # GRU layer with input size `embedding_dim`, hidden size `gru_units`\n",
        "        # and `batch_first=True` to ensure input shape is (batch_size, seq_len, input_size)\n",
        "        self.gru = nn.GRU(embedding_dim, gru_units, batch_first=True)  # GRU layer\n",
        "        self.fc = nn.Linear(gru_units, vocab_size)  # Fully connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # Apply embedding\n",
        "        _, hidden = self.gru(x)  # GRU forward pass (we only need the hidden state)\n",
        "        output = self.fc(hidden.squeeze(0))  # Apply dense layer\n",
        "        return th.softmax(output, dim=-1)  # Apply softmax activation\n",
        "\n",
        "# Model parameters\n",
        "vocab_size = len(vocab)  # Vocabulary size\n",
        "embedding_dim = 128\n",
        "gru_units = 128\n",
        "\n",
        "# Instantiate the model\n",
        "model = GRUModel(vocab_size, embedding_dim, gru_units)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Display model architecture\n",
        "print(model)\n",
        "\n",
        "# Save model path\n",
        "model_path = './wordgen_rnn_gru_model.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gf0V7aiFH4NK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create DataLoader for training and validation sets\n",
        "batch_size = 32\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "# Create DataLoader for validation set\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSiJsWKXH4NK",
        "outputId": "b4c2b125-e1dc-4eee-80e3-01a320aa6bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "Epoch 1/3, Loss: 10.1725, Accuracy: 0.1159\n",
            "Validation Loss: 10.1691, Validation Accuracy: 0.1191\n",
            "Epoch 2/3\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, model_path):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        # Initialize loss and accuracy\n",
        "        epoch_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Training phase\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()  # Clear gradients\n",
        "            output = model(X_batch)  # Forward pass\n",
        "            loss = criterion(output, y_batch)  # Compute loss\n",
        "            loss.backward()  # Backpropagation\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            _, predicted = th.max(output, 1)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "\n",
        "        train_accuracy = correct / total\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with th.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                output = model(X_batch)\n",
        "                loss = criterion(output, y_batch)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = th.max(output, 1)\n",
        "                val_correct += (predicted == y_batch).sum().item()\n",
        "                val_total += y_batch.size(0)\n",
        "\n",
        "        val_accuracy = val_correct / val_total\n",
        "        print(f\"Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "        model.train()  # Switch back to training mode\n",
        "\n",
        "    # Save the trained model\n",
        "    th.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=3, model_path=model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HX3XCNWcH4NL",
        "outputId": "f52f011a-fd50-427a-b70b-3a0bffa8dae4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GRUModel(\n",
            "  (embedding): Embedding(29377, 128)\n",
            "  (gru): GRU(128, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=29377, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# Step 5: Load the Saved Model\n",
        "# ==========================================\n",
        "# Load the saved model\n",
        "loaded_model = GRUModel(vocab_size, embedding_dim, gru_units)\n",
        "loaded_model.load_state_dict(th.load(model_path))\n",
        "loaded_model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Verify the model was loaded correctly\n",
        "print(loaded_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Hisxe4nH4NL",
        "outputId": "86b10a90-0a2b-4399-ed49-a76bf008f0d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed Text: The project gutenberg ebook of\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating text: 100%|ââââââââââ| 15/15 [00:00<00:00, 548.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text: the project gutenberg ebook of death langley demonstrable gogs-wouns lubberly squash single inwardly neighs churlish sar senators commend keeper-back flush\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generate_text(model, vocab, seed_text, max_len, num_words=50, temperature=0.8):\n",
        "    \"\"\"\n",
        "    Generate text using the trained PyTorch model with a temperature parameter.\n",
        "    :param model: Trained PyTorch model.\n",
        "    :param vocab: Vocabulary object for token-to-index and index-to-token mapping.\n",
        "    :param seed_text: Initial input text to start generation.\n",
        "    :param max_len: Maximum sequence length for padding.\n",
        "    :param num_words: Number of words to generate.\n",
        "    :param temperature: Temperature value for controlling randomness.\n",
        "    :return: Generated text string.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    # Check if the seed text is empty\n",
        "    if not seed_text:\n",
        "        raise ValueError(\"Seed text cannot be empty.\")\n",
        "    # Check if the seed text contains only valid tokens\n",
        "    # Tokenize the seed text\n",
        "    # Convert the seed text to lowercase and split into words\n",
        "    seed_text = preprocess_text(seed_text)\n",
        "\n",
        "    # Convert the seed text to a list of tokens\n",
        "    tokens = [vocab[token] for token in seed_text.split() if token in vocab]  # Tokenize seed text\n",
        "\n",
        "    #Convert sequences into words\n",
        "    reverse_vocab = {idx: token for token, idx in vocab.items()}\n",
        "    for _ in tqdm(range(num_words), desc=\"Generating text\"):\n",
        "        # Pad the sequence to the maximum length\n",
        "        if len(tokens) < max_len:\n",
        "            padded_tokens = [0] * (max_len - len(tokens)) + tokens  # Left-pad with zeros\n",
        "        else:\n",
        "            padded_tokens = tokens[-max_len:]  # Truncate to the last `max_len` tokens\n",
        "\n",
        "        # Convert to tensor\n",
        "        input_tensor = torch.tensor([padded_tokens], dtype=torch.long)\n",
        "\n",
        "        # Predict the next word\n",
        "        with torch.no_grad():\n",
        "            predictions = model(input_tensor)  # Forward pass\n",
        "            predictions = predictions[-1]  # Get predictions for the last time step\n",
        "            predictions = predictions / temperature  # Apply temperature scaling\n",
        "            probabilities = torch.softmax(predictions, dim=-1).numpy()  # Convert to probabilities\n",
        "            probabilities = np.clip(probabilities, 1e-10, 1.0)  # Avoid zero probabilities\n",
        "\n",
        "            # Normalize probabilities\n",
        "            probabilities /= np.sum(probabilities)  # Ensure probabilities sum to 1\n",
        "            # Sample the next word index\n",
        "            predicted_word_index = np.random.choice(len(probabilities), p=probabilities)\n",
        "\n",
        "            # Check if the predicted index is valid\n",
        "            if predicted_word_index < 0 or predicted_word_index >= len(vocab):\n",
        "                raise ValueError(f\"Predicted word index {predicted_word_index} is out of bounds.\")\n",
        "            # Convert the predicted index to a word\n",
        "            predicted_word = reverse_vocab[predicted_word_index]\n",
        "\n",
        "            # Check if the predicted word is valid\n",
        "            if predicted_word not in vocab:\n",
        "                raise ValueError(f\"Predicted word '{predicted_word}' is not in the vocabulary.\")\n",
        "            # Append the predicted word to the seed text\n",
        "            tokens.append(predicted_word_index)\n",
        "            seed_text += ' ' + predicted_word\n",
        "\n",
        "    return seed_text\n",
        "\n",
        "# Example of text generation\n",
        "th.manual_seed(42)  # Set random seed for reproducibility\n",
        "seed_text = \"The project gutenberg ebook of\"\n",
        "print(\"Seed Text:\", seed_text)\n",
        "max_len = 10\n",
        "generated_text = generate_text(model, vocab, seed_text, max_len, num_words=15, temperature=0.9)\n",
        "#Save the generated text to a file\n",
        "with open(\"generated_word.txt\", \"w\") as f:\n",
        "    f.write(generated_text)\n",
        "# Print the generated text\n",
        "print(\"Generated Text:\", generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxinClHJH4NM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}